{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection - VGG9 CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for training Emotion Dection model \n",
    "- Data Augmentation(rotation, zoom, flip etc) applied\n",
    "- Rescale/ normalize the input image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "num_classes = 7\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = './train'\n",
    "validation_data_dir = './validation'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Keras CNN VGG9 Model (LittleVGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "\n",
    "\n",
    "def cnn_VGG9_model(activation=\"relu\", kernel_initializer=\"he_normal\"):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=kernel_initializer,\n",
    "                     input_shape = (img_rows, img_cols, 1)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=kernel_initializer, \n",
    "                     input_shape = (img_rows, img_cols, 1)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block2: CONV => RELU => CONV => RELU => POOL\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block3: CONV => RELU => CONV => RELU => POOL\n",
    " \n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 4: CONV => RELU => CONV => RELU => POOL\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 5: FC => RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Block 6: FC => RELU layers\n",
    "    model.add(Dense(64, kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Block 7: softmax classifier\n",
    "    model.add(Dense(num_classes, kernel_initializer=kernel_initializer))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def model_train(model, nb_train_samples, nb_validation_samples, epochs, CHECKPOINT_PATH):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(CHECKPOINT_PATH,\n",
    "                                 monitor=\"val_loss\",\n",
    "                                 mode=\"min\",\n",
    "                                 save_best_only = True,\n",
    "                                 verbose=1)\n",
    "\n",
    "    earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                              min_delta = 0, \n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "    # we put our call backs into a callback list\n",
    "    callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "    # We use a very small learning rate \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = Adam(lr=0.001),\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch = nb_train_samples // batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks = callbacks,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = nb_validation_samples // batch_size)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, nb_train_samples, nb_validation_samples):\n",
    "\n",
    "    # We need to recreate our validation generator with shuffle = false\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "            validation_data_dir,\n",
    "            color_mode = 'grayscale',\n",
    "            target_size=(img_rows, img_cols),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "\n",
    "    class_labels = validation_generator.class_indices\n",
    "    class_labels = {v: k for k, v in class_labels.items()}\n",
    "    classes = list(class_labels.values())\n",
    "\n",
    "\n",
    "    #Confution Matrix and Classification Report\n",
    "    Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    return y_pred, validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1767/1767 [==============================] - 720s 408ms/step - loss: 2.0288 - accuracy: 0.2072 - val_loss: 1.8902 - val_accuracy: 0.2517\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.89023, saving model to ./emotion_vgg9.h5\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "CHECKPOINT_PATH = \"./emotion_vgg9.h5\"\n",
    "\n",
    "epochs = 1\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3589\n",
    "view_model_parameters = False\n",
    "view_confusion_matrix = False\n",
    "\n",
    "model = cnn_VGG9_model(activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "if view_model_parameters:\n",
    "    print(model.summary())\n",
    "\n",
    "model_train(model, nb_train_samples, nb_validation_samples, epochs, CHECKPOINT_PATH)\n",
    "\n",
    "y_pred, validation_generator = model_predict(model, nb_train_samples, nb_validation_samples)\n",
    "\n",
    "\n",
    "if view_confusion_matrix:\n",
    "    \n",
    "    print('Confusion Matrix & Classification Report : ')\n",
    "    print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "    target_names = list(class_labels.values())\n",
    "    print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "    plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    _ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "    _ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation on some random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "classifier = load_model('./emotion_vgg9.h5')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction : Realtime on our webcam\n",
    "- Haarcascades model has been used for face detection in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from time import sleep\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "classifier = load_model('./emotion_vgg9.h5')\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48,48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if time.time() - start_time > 60 :\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
